{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Сентимент-анализ отзывов на товары (простая версия)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выполним классификацию отзывов по тональности на готовой обучающей выборке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords as nltk_stop_words\n",
    "from nltk.corpus import words\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#from sklearn.model_selection import cross_val_score\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import numpy as np\n",
    "from time import time\n",
    "from sklearn.grid_search import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определим необходимые функции. \n",
    "Чтение данных и разделение на обучающую и тестовую выборки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    X_train = []\n",
    "    X_test = []\n",
    "    id_test = []\n",
    "    y_train = []\n",
    "    with open('products_sentiment_train.tsv') as f:\n",
    "        for line in f:\n",
    "            parts = line.rsplit('\\t', 1)\n",
    "\n",
    "            X_train.append(parts[0].strip())\n",
    "            y_train.append(parts[1].strip())\n",
    "    \n",
    "    with open('products_sentiment_test.tsv') as f:\n",
    "        f.readline()\n",
    "        for line in f:\n",
    "            parts = line.split('\\t', 1)\n",
    "            id_test.append(parts[0].strip())\n",
    "            X_test.append(parts[1].strip())                    \n",
    "\n",
    "    return X_train, y_train, id_test, X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучение и классификация выбранным классификатором, сохранение результатов в файл с временной меткой в названии для удобства."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(predictor, data_train, y, id_test, data_test, cv_score=None):\n",
    "    predictor.fit(data_train, y)\n",
    "    prediction = predictor.predict(data_test)\n",
    "    #print predictor\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "    filepath_prediction = 'data/prediction-%s-data.csv' % timestamp\n",
    "    filepath_description = 'data/prediction-%s-estimator.txt' % timestamp\n",
    "\n",
    "    # Create a dataframe with predictions and write it to CSV file   \n",
    "    predictions_df = pd.DataFrame(data=prediction, columns=['y'])\n",
    "    predictions_df.to_csv(filepath_prediction, sep=',', index_label='Id')\n",
    "\n",
    "    # Write a short description of the classifier that was used\n",
    "    f = open(filepath_description, 'w')\n",
    "    f.write(str(predictor))\n",
    "    score = '\\nCross-validation score %.8f' % cv_score    \n",
    "    f.write(score)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее описаны различные конфигурации для выделения признаков и классификации, использовавшиеся в экспериментах.\n",
    "Для каждой конфигурации также указывается набор параметров и перечень значений, использовавшихся для подбора наилучшего сочетания."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вариант-1 с признаками на основе счетчика слов и классификация логистической регрессией."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_pipeline_and_params_1():\n",
    "    pipeline = Pipeline([\n",
    "        ('vect', CountVectorizer()),\n",
    "        ('logreg', LogisticRegression()),\n",
    "    ])\n",
    "    parameters = {\n",
    "        'vect__max_df': (0.6, 0.8, 1.0),\n",
    "        'vect__min_df': (0, 1, 2, 5),\n",
    "        'vect__stop_words': ('english', None),\n",
    "        'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams        \n",
    "        'logreg__C': (0.0001, 0.01, 1),\n",
    "        'logreg__penalty': ('l2', 'l1'),        \n",
    "    }\n",
    "    return pipeline, parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вариант-2 с признаками на основе частотности слов (TF-IDF) и классификация логистической регрессией."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_pipeline_and_params_2():\n",
    "    pipeline = Pipeline([\n",
    "        ('tfidf', TfidfVectorizer()),\n",
    "        ('logreg', LogisticRegression()),\n",
    "    ])\n",
    "    parameters = {\n",
    "        'tfidf__max_df': (0.6, 0.8, 1.0),\n",
    "        'tfidf__min_df': (0, 5, 10, 15),\n",
    "        'tfidf__ngram_range': ((1, 1), (1, 2), (1,3), (2,3)),  # unigrams or bigrams\n",
    "        #'tfidf__use_idf': (True, False),\n",
    "        'tfidf__norm': ('l1', 'l2'),   \n",
    "        'logreg__C': (0.0001, 0.01, 1),\n",
    "        'logreg__penalty': ('l2', 'l1'),  \n",
    "    }\n",
    "    return pipeline, parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вариант-3 с признаками на основе частотности слов (TF-IDF) и классификатор на основе стохастического градиентного спуска."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_pipeline_and_params_3():\n",
    "    # gives 0.7895 on cross-validation and 0.835 on Kaggle. done in 108.629s\n",
    "    pipeline = Pipeline([\n",
    "        ('vect', CountVectorizer()),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', SGDClassifier()),\n",
    "    ])\n",
    "    parameters = {\n",
    "        'vect__max_df': (0.5, 0.75, 1.0),\n",
    "        #'vect__max_features': (None, 5000, 10000, 50000),\n",
    "        'vect__ngram_range': ((1, 3), (1, 2)),  # unigrams or bigrams\n",
    "        #'tfidf__use_idf': (True, False),\n",
    "        'tfidf__norm': ('l1', 'l2'),\n",
    "        'clf__alpha': (0.00001, 0.000001),\n",
    "        'clf__penalty': ('l2', 'elasticnet'),\n",
    "        'clf__n_iter': (10, 50, 80),\n",
    "    }\n",
    "    return pipeline, parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вариант-4 с признаками на основе частотности слов (TF-IDF) и классификация методом опорных векторов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_pipeline_and_params_4():\n",
    "    nltk_sw = nltk_stop_words.words('english')\n",
    "    pipeline = Pipeline([\n",
    "        ('vect', CountVectorizer()),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', LinearSVC()),\n",
    "    ])\n",
    "    parameters = {\n",
    "        #'vect__max_df': (0.8, 0.9, 1),\n",
    "        'vect__min_df': (0, 5, 10),\n",
    "        #'vect__vocabulary': (None),\n",
    "        'vect__stop_words': ('english', nltk_sw, None),\n",
    "        'vect__max_features': (None, 5000, 10000, 50000),\n",
    "        #'vect__analyzer' : ('word', 'char', 'char_wb',), \n",
    "        'vect__ngram_range': ((1,1), (1, 4), (1, 3)),  # unigrams or bigrams\n",
    "        #'tfidf__use_idf': (True, False),\n",
    "        #'tfidf__smooth_idf': (True, False),\n",
    "        #'tfidf__norm': ('l1', 'l2'),\n",
    "        #'tfidf__sublinear_tf': (True, False),\n",
    "        #'clf__C': (0.00001, 0.001, 0.1, 1),        \n",
    "    }\n",
    "    return pipeline, parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вариант-5 с признаками на основе частотности слов (TF-IDF) и классификатор на основе стохастического градиентного спуска с использванием корпуса английских слов, стоп-слов и анализатора частей слов (параметр \"char_wb\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_pipeline_and_params_5():\n",
    "    wordlist = set(words.words())  \n",
    "    nltk_sw = nltk_stop_words.words('english')\n",
    "    pipeline = Pipeline([\n",
    "        ('vect', CountVectorizer(vocabulary=wordlist)),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', SGDClassifier()),\n",
    "    ])\n",
    "    parameters = {\n",
    "        'vect__strip_accents': ('ascii', None),\n",
    "        'vect__analyzer': ('word', 'char_wb'),\n",
    "        'vect__max_df': (0.8, 1.0),\n",
    "        #'vect__min_df': (0, 1, 2),\n",
    "        #'vect__max_features': (100, 200, 500, 1000, 2000, 5000),\n",
    "        'vect__vocabulary': (None, wordlist),\n",
    "        'vect__stop_words': ('english', nltk_sw, None),\n",
    "        'vect__ngram_range': ((1, 3), (1, 1), (1, 2)),\n",
    "        'tfidf__use_idf': (True,),\n",
    "        'tfidf__norm': ('l1',),\n",
    "        'clf__alpha': (0.000001,),\n",
    "        'clf__penalty': ('l1',),\n",
    "        #'clf__n_iter': (10, 50, 80),\n",
    "    }\n",
    "    return pipeline, parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция перебора комбинаций параметров и определения наилучшей конфигурации с помощью GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def do_grid_search(pipeline, parameters, X_train, y_train):\n",
    "    grid_search = GridSearchCV(pipeline, parameters, scoring='accuracy')\n",
    "    t0 = time()\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    print \"done in %0.3fs\" % (time() - t0)\n",
    "    \n",
    "    print(\"Best score: %.4f\" % grid_search.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    return grid_search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Основная фунция, описывающая отдельный эксперимент: сформировать тестируюмую конфигирацию и наборы параметров, выполнить перебор для определения наилучших и выполнить классификацию тестовых данных с помощью лучшего классификатора."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def do_experiment(X_train, y_train, id_test, X_test, get_pipeline_and_params):      \n",
    "    pipeline, parameters = get_pipeline_and_params()\n",
    "    gs = do_grid_search(pipeline, parameters, X_train, y_train)\n",
    "    predict(gs.best_estimator_, X_train, y_train, id_test, X_test, gs.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтение исходных данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, y_train, id_test, X_test = read_data() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выполнение серии экспериментов, их результаты по точности при кросс-валидации и подобранные значения параметров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 91.633s\n",
      "Best score: 0.7740\n",
      "Best parameters set:\n",
      "\tlogreg__C: 1\n",
      "\tlogreg__penalty: 'l2'\n",
      "\tvect__max_df: 0.8\n",
      "\tvect__min_df: 0\n",
      "\tvect__ngram_range: (1, 1)\n",
      "\tvect__stop_words: None\n"
     ]
    }
   ],
   "source": [
    "do_experiment(X_train, y_train, id_test, X_test, get_pipeline_and_params_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 303.685s\n",
      "Best score: 0.7580\n",
      "Best parameters set:\n",
      "\tlogreg__C: 1\n",
      "\tlogreg__penalty: 'l2'\n",
      "\ttfidf__max_df: 0.6\n",
      "\ttfidf__min_df: 0\n",
      "\ttfidf__ngram_range: (1, 1)\n",
      "\ttfidf__norm: 'l2'\n"
     ]
    }
   ],
   "source": [
    "do_experiment(X_train, y_train, id_test, X_test, get_pipeline_and_params_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 108.629s\n",
      "Best score: 0.7895\n",
      "Best parameters set:\n",
      "\tclf__alpha: 1e-06\n",
      "\tclf__n_iter: 10\n",
      "\tclf__penalty: 'l2'\n",
      "\ttfidf__norm: 'l1'\n",
      "\tvect__max_df: 0.5\n",
      "\tvect__ngram_range: (1, 3)\n"
     ]
    }
   ],
   "source": [
    "do_experiment(X_train, y_train, id_test, X_test, get_pipeline_and_params_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 54.368s\n",
      "Best score: 0.7840\n",
      "Best parameters set:\n",
      "\tvect__max_features: 10000\n",
      "\tvect__min_df: 0\n",
      "\tvect__ngram_range: (1, 3)\n",
      "\tvect__stop_words: None\n"
     ]
    }
   ],
   "source": [
    "do_experiment(X_train, y_train, id_test, X_test, get_pipeline_and_params_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 654.527s\n",
      "Best score: 0.7850\n",
      "Best parameters set:\n",
      "\tclf__alpha: 1e-06\n",
      "\tclf__penalty: 'l1'\n",
      "\ttfidf__norm: 'l1'\n",
      "\ttfidf__use_idf: True\n",
      "\tvect__analyzer: 'word'\n",
      "\tvect__max_df: 1.0\n",
      "\tvect__ngram_range: (1, 3)\n",
      "\tvect__stop_words: None\n",
      "\tvect__strip_accents: 'ascii'\n",
      "\tvect__vocabulary: None\n"
     ]
    }
   ],
   "source": [
    "do_experiment(X_train, y_train, id_test, X_test, get_pipeline_and_params_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результаты при кросс-валидации на обучающей выборке, разумеется, отличались от результатов на тестовой выборке, полученных на сайте Kaggle.\n",
    "\n",
    "На тестовых данных лучший результат показала конфигурация, описанная в варианте-3 - признаки по TF-IDF и классификация с помощью SGDClassifier. Результат на Kaggle составил 0.835."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
