# Определение токсичности комментариев

## Описание проекта

Интернет-магазин «Викишоп» запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию.

## Данные

Предоставлены данные о комментариях:
- Текст
- Токсичность

## Задача

Обучите модель классифицировать комментарии на позитивные и негативные. В вашем распоряжении набор данных с разметкой о токсичности правок.

Постройте модель со значением метрики качества F1 не меньше 0.75.

## Используемые библиотеки
*Python*, *Pandas*, *BERT*, *nltk*, *tf-idf*

## Шаги

1. Подготовка
- Загрузка данных
- Подготовка признаков
- Лемматизация
- Разделение на выборки
- Изменение баланса классов
- Изменение весов классов
2. Обучение
- Классификатор LogisticRegression
- Классификатор DecisionTreeClassifier
- Классификатор CatBoostClassifier
- Классификатор SGDClassifier
- Классификатор LightGBM

## Выводы

В ходе работы над проектом было сделано:

- Подготовленны данные обучения на моделях.
- Выбран способ баланса классов и поделены данные на обучающую, валидационную и тестовою выборку.
- Обучены модели и выбраны лучшие из них на валидационной выборке.
- Показаны параметры качества моделей.

Исходные данные обладают большим количеством признаков. Созданных столбцов больше, чем записей данных. Так как TF-IDF превращают текст в численные значения, лучшими моделями стали LogisticRegression, SGDClassifier и LGBMClassifier. CatBoostClassifier может показать себя очень хорошо при долгом обучении на данных. В ходе тестов данный классификатор мог обучатся до 5 часов.

На тестовой выбоке по метрике F1 лучше всего себя показал LGBMClassifier всего на 0.77. Данная модель обладает больними показателями Precision и Accuracy. Это говорит нам, что токсичные комментарии находятся лучше.
